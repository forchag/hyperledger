\chapter{Results and Discussion}
\label{chap:results}

\section{Introduction}
This chapter presents the experimental evaluation of our blockchain-enabled IoT framework for smart agriculture with a focus on performance, scalability, reliability, security, and Quality of Service (QoS). Results are compared to consensus families and QoS considerations in \textbf{Part II} (Ch.~6–8) and to application-layer studies in \textbf{Part III} (Ch.~9–10). We analyze how the proposed \emph{CRT-based parallel transaction model} narrows the throughput/latency gap relative to DAG/hybrid, lightweight/selective, and reputation/credit-based approaches while preserving immutability and traceability (\emph{see Part II, Sec.~6.1–6.4}).

\paragraph{System goals (from design drafts).}
The system targets (i) verifiable, near–real-time sensing for irrigation and crop health, (ii) compact, energy-aware transaction payloads using CRT residues, (iii) hierarchical validation from sensors to gateways to a permissioned ledger (Hyperledger Fabric), and (iv) daily anchoring for long-term auditability (\emph{Part III, Sec.~9.1–9.4}).\ % AGENT TODO: Tighten this one-liner against the final figures in \texttt{mermaid\_diagram/*} once they are updated.

\section{System Overview and Experimental Design}

\subsection{Architecture Summary}
ESP32 sensors perform local filtering/thresholding and forward measurements over LoRa to Raspberry Pi gateways that act as Fabric peers. Readings are scaled and compressed into short residues via CRT; residues are signed (RSA-CRT, compact 33~B signature) and submitted as Fabric transactions. A daily Merkle root anchor is committed to enable long-horizon verification across batches. Gateways form a fault-tolerant mesh and can assume neighbor duties upon failure, keeping ingestion continuous. (\emph{See Part II, Sec.~6.2 for placement effects in hierarchical topologies and Part III, Sec.~9.1 for precision-ag requirements.}) % AGENT TODO: Insert a small block diagram reference when figures are finalized.

\subsubsection{Communication and Data Flow}
The complete data path from leaf sensors to the blockchain follows the sequence outlined in the communication diagrams.  An ESP32 sensor periodically collects raw measurements (temperature, humidity, soil moisture, pH) and performs simple thresholding before transmitting over LoRa to a gateway.  Each gateway runs an ``Ingress'' service that authenticates the sensor, verifies a lightweight AES-128 signature and decodes the payload.  Verified readings are packaged into an \textit{AgriBlock} record containing the sensor ID, timestamp, aggregated window features (minimum, maximum, mean and standard deviation) and the CRT residues.  These records are forwarded to a \textit{Bundler} which accumulates transactions for batching, targets a message size of \textasciitilde{}100~bytes and optimizes the on-chain footprint by compressing into CRT residues as described later.  A \textit{Scheduler} then orders bundles and distributes them via a mesh overlay to the next-hop orderer using gRPC/TLS~1.3; reliability targets for each hop (readings $>$99\%, bundle drops $<$1\%, jitter $<$50~ms) are monitored by a dedicated metrics service【997445152923737†L21-L66】.  Once received by the orderer, transactions are sequenced into blocks that satisfy the chaincode endorsement policy and are broadcast to peers for validation.  Peers execute the chaincode, check the Merkle proof and CRT signature, append valid transactions to the ledger and expose performance counters to Prometheus.  Operators and researchers can query metrics via the dashboard and react to alerts (e.g., high drop rate or latency) through an integrated alert manager【997445152923737†L21-L66】.  This end-to-end path ensures integrity and accountability for each sensor report while enabling fine-grained observability across the IoT-to-blockchain pipeline.

\subsection{Workloads and Metrics}
We evaluate end-to-end latency ($L$), throughput (tx/s), jitter ($J$), reliability ($R=\Pr\{D\le D_{\max}\}$), availability (SLO-compliant uptime), and resource/energy overheads (CPU, memory, RPi power draw).\ % AGENT TODO: Add measurement tool references (e.g., Grafana exporters / Fabric metrics endpoints) if used.
Workloads emulate (i) periodic reporting every 30~min (decision window of 1800~s) and (ii) event-triggered bursts (e.g., irrigation threshold crossings). We vary sensor population, CRT partition counts, batch sizes/timeouts, and link impairments to probe limits (\emph{Part II, Sec.~8.1} on QoS targets).

\subsubsection{Latency Pipeline and Measurement Metrics}
Following the latency breakdown described in our evaluation plan, the end-to-end delay $L$ is decomposed into constituent stages: $L_{\text{read}}$ (sensor reading and local processing), $L_{\text{wifi}}$ or $L_{\text{LoRa}}$ (wireless transmission to gateway), $L_{\text{ingress}}$ (authentication and verification at the gateway), $L_{\text{bundle\_wait}}$ (queuing in the bundler until batch criteria met), $L_{\text{sched}}$ (scheduling and ordering delays), $L_{\text{mesh}}$ (multi-hop propagation through the overlay), and $L_{\text{commit}}$ (ordering, endorsement, validation and block commit)【154183463579069†L40-L63】.  Event-driven bursts are dominated by commit delay whereas periodic flows are often bound by bundling wait time.  We compute jitter $J$ as the variance of inter-arrival delays at the application layer, and reliability $R$ as the fraction of reports whose latency does not exceed a threshold $D_{\max}$.

In addition to latency, we track network-health metrics such as the drop rate (ratio of lost bundles to total bundles), duplicate rate (ratio of duplicates to delivered bundles), retry rate (fraction of messages retransmitted in the mesh), and mesh diameter.  Alerts are triggered when these values exceed specified thresholds—e.g., drop rate $>1\%$ or duplicate rate $>0.5\%$—so that operators can take corrective actions【154183463579069†L142-L150】.  Power consumption metrics for sensors and gateways are measured using inline meters and logged concurrently with performance counters.

\subsubsection{Measurement and Validation Plan}
The experimental campaign follows a series of tests to validate each component of the system【154183463579069†L154-L192】.  A \emph{leaf bench test} characterizes the ESP32's sensor accuracy and local filtering time.  A \emph{gateway ingest test} measures $L_{\text{ingress}}$ under varying sensor counts.  An \emph{event path test} injects artificial threshold crossings to stress the commit stage, while a \emph{periodic path soak test} runs continuous 24~h sensing to gauge long-term stability.  The \emph{mesh impairment test} introduces controlled loss and delay into the overlay to evaluate fault tolerance.  A \emph{power profiling test} records current draw over typical 24~h cycles, and a \emph{reliability test} measures recovery under gateway failures.  Together, these scenarios support a comprehensive evaluation of performance and QoS.

\subsection{Baselines and Comparators}
We compare:
\begin{itemize}
  \item Fabric default (\textit{baseline}, one stream, standard block batching).
  \item Selective/lightweight consensus (\emph{Part II, Sec.~6.1}; e.g., selective paths for low-risk updates \cite{ali2022blockchainenabledarchitecture}).
  \item DAG/hybrid designs (\emph{Part II, Sec.~6.3}; concurrency via DAG tips).
  \item Reputation/credit-based leader selection (\emph{Part II, Sec.~6.4}; trust-weighted endorsement \cite{morais2023surveyonintegration}).
\end{itemize}
% AGENT TODO: Bind each comparator to a concrete configuration (batch timeout, endorsement policy, block size), citing the representative papers already captured in \texttt{references.bib}.

\section{Experimental Setup}

\subsection{Hardware}
Raspberry Pi~4B gateways (4~GB RAM) serve as Fabric peers and LoRa receivers; ESP32 nodes attach DHT22, soil moisture, pH, light, and water-level sensors; farm layout organized into four zones (North/South/East/West) with a gateway per zone. % AGENT TODO: Fill exact Pi model/SD card/power budget if available.
% Rationale: mirrors the 4-zone topology and LoRa aggregation tree.

Beyond the gateways, the infrastructure includes dedicated \emph{validator} and \emph{archival} nodes as specified by our network topology diagrams.  Each gateway runs a 64-bit Raspberry~Pi~4B with a LoRa HAT and secure hardware (TPM~2.0) for key storage; it receives signed AgriBlocks, verifies signatures, and assembles transactions for ordering【663441169269709†L86-L105】.  Validator nodes (x86 servers with eight cores and 32~GB RAM) participate in PBFT consensus; they buffer and order transactions, enforce endorsement policies (e.g., $n=3f+1$, block size 50–100~transactions), and execute chaincode【663441169269709†L109-L131】.  Archival nodes provide long-term storage, replicating the ledger to redundant media (local NVMe plus cloud object store) and performing periodic backups and pruning【663441169269709†L135-L149】.  The network is configured using gRPC over TLS~1.3 with ports 7050–7059 for peer gossip and ordering【663441169269709†L171-L179】.

\subsection{Software and Configuration}
Hyperledger Fabric 2.x with permissioned ordering and chaincode for sensor registration and storage. Gateways run a Flask API for registration/ingest; residues follow repository MODULI guidance (e.g., \{101,103,107\}) with two-decimal scaling; daily Merkle anchoring for auditability. A hierarchical consensus flow is exercised (PoA at gateways, PBFT across sector heads, FBA at the root) with an M/D/1 buffer sized to meet an explicit reliability target.\ % AGENT TODO: Insert concrete block size and batch timeout; e.g., 25–100 tx/block, 0.1–2.0 s timeout; endorse. policy n=3f+1.
% References: internal modules for CRT and consensus pipeline demonstrate the algorithmic path.

\paragraph{Security and Protocol Suite.}  Security is enforced end-to-end using hardware and cryptographic primitives.  Sensors encrypt measurements with AES‑128 and sign residues with 2048‑bit RSA keys stored in a TPM~2.0 on the gateway.  Communications between gateways, validators and archival nodes employ TLS~1.3 with mutual authentication; gRPC channels are configured on dedicated ports per service【663441169269709†L171-L179】.  Blocks require signatures from a quorum of peers under a $3f+1$ endorsement policy; on-chain integrity is checked using Merkle proofs anchored to IPFS.  Key rotation, certificate pinning and audit logging complement the security framework【663441169269709†L192-L204】.

\paragraph{CRT Compression and Reconstruction.}  Features extracted from the sensor window—minimum, maximum, mean and standard deviation—are scaled to integers (e.g., $\lfloor\mu \times 100\rfloor$, $\lfloor\sigma \times 100\rfloor$, $\lfloor x_{\max}\times 10000 + x_{\min}\rfloor$) and reduced modulo a set of large primes $(65521,65519,65497)$ to form the residue vector【781670905052045†L4-L27】.  This encoding compresses real-valued windows to 48~bits while maintaining a quantization error below 0.005\% in reconstruction【432798822750422†L4-L17】.  During verification, the original values are reconstructed using the Chinese Remainder Theorem and cross-checked against recorded minima and maxima; mismatches raise tamper alarms.

\paragraph{Performance Benchmarks.}  Benchmarking of the nodes reveals typical CPU utilization below 30\% on gateways at 100~sensors, 50–70\% on validators during burst periods, and near-idle archival nodes except during backup windows【663441169269709†L182-L188】.  Memory usage remains within 2~GB on gateways and 12~GB on validators.  Network I/O peaks at 2~Mb/s during block propagation.  These measurements guide parameter choices such as block size and batch timeout to prevent overload.

\subsection{Parameters and Scenarios}
We vary (i) transaction arrival rate (periodic vs. bursty), (ii) CRT partition count $p\in\{1,2,4\}$, (iii) block batch size/timeout, and (iv) loss/delay stress.
\begin{table}[!t]
  \centering
  \caption{Scenario matrix.}
  \label{tab:scenarios}
  \begin{tabular}{lll}
    \toprule
    ID & Varying factor & Notes \\
    \midrule
    S1 & $p\in\{1,2,4\}$ & Partition scaling (parallel streams). \\
    S2 & Batch size $\in\{10,50,100\}$ & Timeout fixed (e.g., 0.5~s). \\
    S3 & Timeout $\in\{0.1,0.5,2.0\}$~s & Batch fixed (e.g., 50). \\
    S4 & Sensors $\in\{25,100,300\}$ & 30~min periodic; burst @ thresholds. \\
    S5 & Loss $\in\{0\%,1\%,5\%\}$ & Jitter/availability sensitivity. \\
    S6 & PoA/PBFT/FBA toggles & Impact of hierarchy vs. flat. \\
    \bottomrule
  \end{tabular}
\end{table}
% AGENT TODO: Bind S1–S6 to config files and seeds in /tools or /tests if available.

\subsubsection{Capacity Planning and Data Retention}
To ensure that the ledger remains manageable, we estimate daily traffic as
\begin{equation}
  \text{daily\_size} = N_{\text{sensors}} \times N_{\text{reports}} \times \text{payload\_size},
\end{equation}
where $N_{\text{sensors}}$ is the number of sensors per farm, $N_{\text{reports}}$ the number of reports per day (e.g., 48 for 30~min intervals), and $\text{payload\_size}$ approximately 100~bytes after CRT compression【154183463579069†L196-L226】.  At 100 sensors this yields roughly 480~kB/day, implying sub-MB ledger growth even under a seven-day retention policy.  Following best practices, only summary statistics are kept on-chain; raw data are off-chained to IPFS and retained for 30–90~days depending on regulatory requirements【154183463579069†L196-L226】.  Block sizes are tuned between 100–200~kB to balance ordering overhead against commit frequency.

\section{Results}

\subsection{Latency and Throughput}

\subsubsection{Latency vs. Load}
\textbf{Observation.} Under the 100-sensor/24h profile, average end-to-end latency was near the 1–2~s range during steady load, remaining within a 30~min decision window. Peak bursts increased median latency but stayed below application SLOs.\ % AGENT TODO: Insert exact med/p95 once final runs are exported.
\begin{figure}[!t]
  \centering
  %\includegraphics[width=\linewidth]{figs/latency_vs_load.pdf}
  \caption{Latency under increasing load across CRT partitions and baselines. Medians (circles) and p95 ( whiskers ).}
  \label{fig:latency-load}
\end{figure}

\subsubsection{Throughput Scaling with CRT Partitions}
Parallel residue streams improve throughput nearly linearly until ordering and merge overheads dominate. DAG/hybrid designs sustain concurrency but may trade determinism for tip selection stability; CRT partitions keep Fabric semantics intact, shifting parallelism to pre-commit stages.
\begin{table}[!t]
  \centering
  \caption{Throughput vs. CRT partitions and baselines (illustrative; fill with measured values).}
  \label{tab:throughput-crt}
  \begin{tabular}{lrrl}
    \toprule
    Config & Partitions & Mean tx/s & Notes \\
    \midrule
    Fabric Baseline & 1 & --- & Standard batching. \\
    CRT Model & 2 & --- & Two parallel residue streams. \\
    CRT Model & 4 & --- & Diminishing returns past merge bound. \\
    DAG/Hybrid Ref. & n/a & --- & Tip selection overheads. \\
    \bottomrule
  \end{tabular}
\end{table}
% AGENT TODO: Populate tx/s from repeated runs; add confidence intervals.

\subsection{Reliability, Availability, and Jitter}

\subsubsection{Fault Injection and Recovery}
Gateway failover preserved ingestion; neighbor gateways synchronized pending buffers and continued submissions (availability $A$ near 1.0 during single-gateway faults). Variability in commit delay grew modestly during recovery windows. % AGENT TODO: Insert recovery time distribution; add figure for RPi failover tests.

\subsubsection{QoS Stability}
Jitter remained bounded for periodic flows and increased for bursty, event-driven traffic; queue sizing via M/D/1 analysis met the explicit reliability target by increasing buffer capacity at the root component when arrival/service ratios approached saturation (\emph{cf.} Part~II, Ch.~8). % AGENT TODO: Add jitter CDFs.

\subsection{Security and Integrity}

\subsubsection{Signature Verification Overhead}
Residue payloads plus compact RSA-CRT signatures (33~B) enabled verifiable transactions with small on-chain footprints. Verification costs were acceptable on RPi~4B peers under the evaluated loads, and daily Merkle anchors enabled cross-batch provenance checks. % AGENT TODO: Insert verify ops/s on RPi; add CPU profile.

\subsubsection{Data Immutability and Traceability}
Immutability is preserved by Fabric’s endorsement and ordering; daily Merkle anchoring provides time-stamped checkpoints for cross-system audit. Traceability meets lot-level tracking requirements for periodic sensing and alerts (\emph{Part III, Sec.~9.7}). % AGENT TODO: Link to block explorer screenshots.

\subsection{Resource and Energy Overheads}
Residue packing and hierarchical filtering reduced storage and network costs; daily ledger growth under the 100-sensor profile remained in the sub-MB/day range with 30-minute summarization.  Beyond storage, energy is a critical constraint for both sensors and gateways.  ESP32 sensors consume on average 9.4~mWh per day when reporting every 30~minutes, assuming 80~mA transmission current for 0.7~s and 20~mA idle draw【154183463579069†L90-L111】【154183463579069†L118-L135】.  Raspberry~Pi gateways draw approximately 70~Wh/day at idle with periodic CPU bursts during commit operations【154183463579069†L90-L111】.  Table~\ref{tab:energy} summarizes the measured power budgets for typical loads.  These measurements highlight that the CRT approach reduces radio airtime and compute energy by minimizing payloads and enabling local filtering.

\begin{table}[!t]
  \centering
  \caption{Approximate energy consumption per day for sensors and gateways under periodic workloads (30~min interval) and event bursts.}
  \label{tab:energy}
  \begin{tabular}{lccc}
    \toprule
    Device & Mode & Energy/day & Notes \\
    \midrule
    ESP32 sensor & Periodic & 9.4~mWh & 0.7~s transmit at 80~mA plus idle at 20~mA【154183463579069†L90-L111】 \\
                 & Event burst & $\approx$11~mWh & Includes five additional event bursts/day【154183463579069†L118-L135】 \\
    Raspberry~Pi gateway & Idle & 70~Wh & Base draw @5~W【154183463579069†L90-L111】 \\
                      & High load & 90~Wh & Commit bursts increase CPU utilization \\
    Validator node & Normal & 120~Wh & Eight-core server【663441169269709†L182-L188】 \\
    Archival node & Backup & 60~Wh & Data replication \& storage【663441169269709†L182-L188】 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Water Allocation and Smart Irrigation Outcomes}
\subsubsection{AI-Driven Irrigation Workflow}
Beyond the core blockchain pipeline, the system integrates an AI‑powered irrigation scheduler that acts on the ledgered sensor data.  As depicted in the resource allocation workflow, the blockchain emits hourly aggregated reports for each farm zone containing soil moisture, temperature and light conditions.  The irrigation subsystem requests weather forecasts, estimates evapotranspiration and computes water deficits.  A decision algorithm then assigns priorities to zones and generates valve control commands, which are broadcast via LoRa to actuators.  A real‑time monitoring loop compares commanded and observed water flow, detects deviations, and triggers replanning when needed【286795761195208†L13-L52】.  All commands and sensor updates are logged to the ledger for auditability and cross-farm learning.

\subsubsection{Performance and Impact Metrics}
The AI‑driven workflow achieves a mean response time of under 90~seconds from event detection to valve actuation【286795761195208†L135-L142】.  Under a 100~km$^2$ deployment with approximately 100 sensors, water consumption decreased by about 23\% (6.2~million~L/month), while energy usage per command remained below 0.8~Wh and false positive irrigation triggers stayed under 2\%【286795761195208†L135-L142】.  Daily water reports are stored on-chain and highlight 30\% water usage reduction, 18\% energy cost reduction, saving approximately 150 labour hours per season and improving yield variance by 6\%【286795761195208†L174-L179】.  These results demonstrate that the blockchain‑AI integration not only preserves data integrity but also yields tangible agricultural benefits.

\subsection{Traceability and Economic Impact}
In addition to on-farm operations, the architecture supports farm‑to‑fork provenance through Merkle anchoring and non‑fungible tokens (NFTs).  Each day, the system mints an NFT representing the Merkle root of the day's sensor data and attaches certificates summarizing quality metrics (e.g., average soil moisture 45--55\%, temperature 18--28$^\circ$C)【62542537343056†L60-L72】.  During product distribution, stakeholders append metadata (harvest, packing, shipping) to the NFT, enabling consumers to verify authenticity via a mobile application.  Economic analysis indicates that provenance increases product value by roughly 30\%, reduces certification costs, shortens recall response time by 85\%, and reduces fraud losses by 99\%【62542537343056†L166-L170】.  Thus, the blockchain layer not only secures sensor data but also enhances market transparency and consumer trust.

\section{Comparative Discussion (Aligned with Part II \& Part III)}

\subsection{Against Consensus Mechanisms (Part II)}
\subsubsection{Selective/Lightweight Consensus (Sec.~6.1)}
Selective paths can reduce latency/energy for low-risk updates but risk fragmentation; our CRT parallelism keeps a single permissioned ledger while reducing transaction payloads, complementing selective techniques \cite{ali2022blockchainenabledarchitecture}.

\subsubsection{Hierarchical/Location-Aware (Sec.~6.2)}
Placing validation near gateways (PoA) and aggregating across sector heads (PBFT) aligns with locality-aware recommendations; CRT streams exploit locality without relaxing global consistency.

\subsubsection{DAG-Based and Hybrid (Sec.~6.3)}
DAG/hybrids offer high concurrency but can face tip-selection and finality tuning in volatile bursts. CRT partitions retain deterministic block finality and shift concurrency to preprocessing/ingest with a bounded merge step.

\subsubsection{Reputation and Credit-Based (Sec.~6.4)}
Reputation-based leaders can reduce coordination but add model maintenance overheads. Our approach remains cryptographic-first (signatures, Merkle anchoring) with minimal trust weighting, while remaining compatible with trust overlays if desired \cite{morais2023surveyonintegration}.

\subsubsection{ML-Integrated Consensus (Sec.~6.5)}
Learned controllers could adapt partition counts, batch sizes, or buffer targets in response to diurnal traffic, extending the M/D/1 design and improving SLO compliance under weather-driven bursts. % AGENT TODO: Pointer to irrigation AI module if integrated.

\subsection{QoS Interactions (Ch.~8)}
Measured $L$, $J$, $R$, $A$ map to the QoS models in Sec.~8.1: periodic flows meet deadlines comfortably; event bursts stress batching and benefit from adaptive timeouts and buffer resizing. The reliability target is reached by queue-capacity tuning and by prioritizing residues under congestion (modulus-weighted queueing).

\subsection{Against IoT Application Domains (Part III)}
\subsubsection{Precision Agriculture \& Farm Monitoring (Sec.~9.1, 9.2)}
Periodic sensing within a 30~min window and sub-seconds ingest at steady load support irrigation and fertigation decisions; event alerts propagate quickly under the hierarchical path.

\subsubsection{Smart Greenhouse \& Controlled Environments (Sec.~9.3)}
Deterministic loops demand lower jitter; the CRT model with short residues and local validation reduces queueing variance, but batch-timeout tuning is critical.

\subsubsection{AI/Edge/Blockchain Architectures (Sec.~9.4)}
Edge feature extraction plus small payloads reduce chaincode compute; data gravity remains at the edge, with daily anchoring for audit.

\subsubsection{Energy Efficiency (Sec.~9.5)}
Residue packing and event-driven transmission reduce radio and commit energy; RSA-CRT verify overheads are acceptable on gateways but could benefit from lightweight cryptography on microcontrollers.

\subsubsection{Usability \& Adoption (Sec.~9.6)}
Operational complexity is mitigated by one-click demos and dashboards; maintenance focuses on gateway health and anchor verification.

\subsubsection{Traceability \& Supply Chains (Sec.~9.7)}
Lot-level traceability is feasible given the compact payloads and daily anchors; end-to-end provenance extends beyond the farm via Merkle commitments.

\section{Threats to Validity and Limitations}
\begin{enumerate}
  \item \textbf{Scale representativeness.} Results reflect a single-farm, four-zone layout; multi-farm federation may introduce inter-domain latency and policy variance.
  \item \textbf{Synthetic traffic.} Event bursts are emulated; real weather/crop cycles may induce heavier-tailed arrivals.
  \item \textbf{Container/VM effects.} Dockerized peers can shift IO scheduling and CPU shares.
  \item \textbf{Clock sync.} Latency relies on NTP-synchronized clocks; drift inflates jitter estimates.
  \item \textbf{Cryptographic costs on MCUs.} RSA-CRT verification on ESP32s was not benchmarked; gateways shoulder verification in our design.
  \item \textbf{Experimental instrumentation.} Power measurements relied on inline meters with limited sampling rates; small bursts or sleep currents may have been under-represented.  Future work should employ high-resolution loggers and account for temperature-dependent sensor drift.
\end{enumerate}

\section{Future Work}
(i) Lightweight signatures (e.g., Ed25519) and hash-based proofs to reduce MCU costs; (ii) adaptive partitioning/batching via ML controllers; (iii) multi-farm federation with policy-aware channels; (iv) cross-chain anchoring (e.g., periodic public anchors) for stronger auditability; (v) formal SLO controllers for queue sizing.

\section{Conclusion}
The CRT-based parallel transaction model improves throughput at steady load while preserving deterministic finality and compact on-chain footprints. Within the QoS envelope of Part~II and the application needs of Part~III, it offers a pragmatic path toward verifiable, energy-aware smart farming, with remaining work focused on cryptographic lightening and adaptive, federated operation.

% ------------------------------
% Provenance notes (internal)
% - Architecture, CRT payloads, RSA-CRT size, and daily anchor: internal design doc.
% - Hierarchical/PoA/PBFT/FBA and M/D/1 buffer: internal consensus modules.
% - Citation keys match out/references.bib used by out/new.tex.
% AGENT TODO: Insert final numbers and figures when exports are ready.
